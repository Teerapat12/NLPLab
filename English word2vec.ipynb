{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word2Vec (English Version).</h2>\n",
    "<p>In this notebook, we will try to construct a word2vec from a dataset found here : https://www.kaggle.com/c/quora-question-pairs</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#Import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"dataset/quora_train.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning the data</h2>\n",
    "<p>Let's clean the data and segment them into sentences</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "nltk.data.path.append(\"D:\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( question, remove_stopwords=False ):\n",
    "    \n",
    "    question_text = re.sub(\"[^a-zA-Z]\",\" \", question)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = question_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences( question, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(question.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from dataset q1\n",
      "Finished parseing q1 : 9.87s.\n",
      "Parsing sentences from dataset q2\n",
      "Finished parseing q2 : 20.46s.\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "print(\"Parsing sentences from dataset q1\")\n",
    "start = timeit.default_timer()\n",
    "for question in dataset[\"question1\"]:\n",
    "    sentences += review_to_sentences(str(question), tokenizer)\n",
    "stop = timeit.default_timer()\n",
    "print(\"Finished parseing q1 : \"+str(round(stop - start,2))+\"s.\")\n",
    "print(\"Parsing sentences from dataset q2\")    \n",
    "for question in dataset[\"question2\"]:\n",
    "    sentences += review_to_sentences(str(question), tokenizer)    \n",
    "stop = timeit.default_timer()\n",
    "print(\"Finished parseing q2 : \"+str(round(stop - start,2))+\"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what',\n",
       "  'is',\n",
       "  'the',\n",
       "  'step',\n",
       "  'by',\n",
       "  'step',\n",
       "  'guide',\n",
       "  'to',\n",
       "  'invest',\n",
       "  'in',\n",
       "  'share',\n",
       "  'market',\n",
       "  'in',\n",
       "  'india'],\n",
       " ['what',\n",
       "  'is',\n",
       "  'the',\n",
       "  'story',\n",
       "  'of',\n",
       "  'kohinoor',\n",
       "  'koh',\n",
       "  'i',\n",
       "  'noor',\n",
       "  'diamond']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-04-28 07:31:04,221 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-04-28 07:31:04,282 : INFO : collecting all words and their counts\n",
      "2017-04-28 07:31:04,284 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-28 07:31:04,314 : INFO : PROGRESS: at sentence #10000, processed 99111 words, keeping 10771 word types\n",
      "2017-04-28 07:31:04,349 : INFO : PROGRESS: at sentence #20000, processed 198901 words, keeping 15372 word types\n",
      "2017-04-28 07:31:04,372 : INFO : PROGRESS: at sentence #30000, processed 297847 words, keeping 18786 word types\n",
      "2017-04-28 07:31:04,397 : INFO : PROGRESS: at sentence #40000, processed 396378 words, keeping 21570 word types\n",
      "2017-04-28 07:31:04,434 : INFO : PROGRESS: at sentence #50000, processed 495897 words, keeping 23996 word types\n",
      "2017-04-28 07:31:04,464 : INFO : PROGRESS: at sentence #60000, processed 595251 words, keeping 26209 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-28 07:31:04,487 : INFO : PROGRESS: at sentence #70000, processed 693633 words, keeping 28128 word types\n",
      "2017-04-28 07:31:04,518 : INFO : PROGRESS: at sentence #80000, processed 793440 words, keeping 29888 word types\n",
      "2017-04-28 07:31:04,553 : INFO : PROGRESS: at sentence #90000, processed 892903 words, keeping 31430 word types\n",
      "2017-04-28 07:31:04,576 : INFO : PROGRESS: at sentence #100000, processed 992033 words, keeping 32934 word types\n",
      "2017-04-28 07:31:04,608 : INFO : PROGRESS: at sentence #110000, processed 1091774 words, keeping 34379 word types\n",
      "2017-04-28 07:31:04,637 : INFO : PROGRESS: at sentence #120000, processed 1190774 words, keeping 35739 word types\n",
      "2017-04-28 07:31:04,665 : INFO : PROGRESS: at sentence #130000, processed 1289539 words, keeping 36976 word types\n",
      "2017-04-28 07:31:04,695 : INFO : PROGRESS: at sentence #140000, processed 1389467 words, keeping 38253 word types\n",
      "2017-04-28 07:31:04,729 : INFO : PROGRESS: at sentence #150000, processed 1488913 words, keeping 39433 word types\n",
      "2017-04-28 07:31:04,758 : INFO : PROGRESS: at sentence #160000, processed 1588691 words, keeping 40578 word types\n",
      "2017-04-28 07:31:04,790 : INFO : PROGRESS: at sentence #170000, processed 1688473 words, keeping 41718 word types\n",
      "2017-04-28 07:31:04,815 : INFO : PROGRESS: at sentence #180000, processed 1787963 words, keeping 42857 word types\n",
      "2017-04-28 07:31:04,844 : INFO : PROGRESS: at sentence #190000, processed 1887555 words, keeping 43853 word types\n",
      "2017-04-28 07:31:04,876 : INFO : PROGRESS: at sentence #200000, processed 1986905 words, keeping 44860 word types\n",
      "2017-04-28 07:31:04,904 : INFO : PROGRESS: at sentence #210000, processed 2086787 words, keeping 45826 word types\n",
      "2017-04-28 07:31:04,930 : INFO : PROGRESS: at sentence #220000, processed 2185048 words, keeping 46791 word types\n",
      "2017-04-28 07:31:04,962 : INFO : PROGRESS: at sentence #230000, processed 2283972 words, keeping 47692 word types\n",
      "2017-04-28 07:31:04,990 : INFO : PROGRESS: at sentence #240000, processed 2382689 words, keeping 48528 word types\n",
      "2017-04-28 07:31:05,019 : INFO : PROGRESS: at sentence #250000, processed 2482567 words, keeping 49427 word types\n",
      "2017-04-28 07:31:05,049 : INFO : PROGRESS: at sentence #260000, processed 2582084 words, keeping 50217 word types\n",
      "2017-04-28 07:31:05,080 : INFO : PROGRESS: at sentence #270000, processed 2682208 words, keeping 51022 word types\n",
      "2017-04-28 07:31:05,117 : INFO : PROGRESS: at sentence #280000, processed 2781482 words, keeping 51871 word types\n",
      "2017-04-28 07:31:05,155 : INFO : PROGRESS: at sentence #290000, processed 2880857 words, keeping 52687 word types\n",
      "2017-04-28 07:31:05,189 : INFO : PROGRESS: at sentence #300000, processed 2980885 words, keeping 53439 word types\n",
      "2017-04-28 07:31:05,224 : INFO : PROGRESS: at sentence #310000, processed 3080129 words, keeping 54174 word types\n",
      "2017-04-28 07:31:05,256 : INFO : PROGRESS: at sentence #320000, processed 3179419 words, keeping 54939 word types\n",
      "2017-04-28 07:31:05,293 : INFO : PROGRESS: at sentence #330000, processed 3278142 words, keeping 55691 word types\n",
      "2017-04-28 07:31:05,334 : INFO : PROGRESS: at sentence #340000, processed 3377418 words, keeping 56449 word types\n",
      "2017-04-28 07:31:05,367 : INFO : PROGRESS: at sentence #350000, processed 3477241 words, keeping 57137 word types\n",
      "2017-04-28 07:31:05,398 : INFO : PROGRESS: at sentence #360000, processed 3576792 words, keeping 57883 word types\n",
      "2017-04-28 07:31:05,424 : INFO : PROGRESS: at sentence #370000, processed 3675881 words, keeping 58548 word types\n",
      "2017-04-28 07:31:05,459 : INFO : PROGRESS: at sentence #380000, processed 3775009 words, keeping 59156 word types\n",
      "2017-04-28 07:31:05,487 : INFO : PROGRESS: at sentence #390000, processed 3874933 words, keeping 59726 word types\n",
      "2017-04-28 07:31:05,514 : INFO : PROGRESS: at sentence #400000, processed 3973785 words, keeping 60348 word types\n",
      "2017-04-28 07:31:05,552 : INFO : PROGRESS: at sentence #410000, processed 4073329 words, keeping 60963 word types\n",
      "2017-04-28 07:31:05,580 : INFO : PROGRESS: at sentence #420000, processed 4173251 words, keeping 61581 word types\n",
      "2017-04-28 07:31:05,613 : INFO : PROGRESS: at sentence #430000, processed 4273533 words, keeping 62251 word types\n",
      "2017-04-28 07:31:05,650 : INFO : PROGRESS: at sentence #440000, processed 4373736 words, keeping 62833 word types\n",
      "2017-04-28 07:31:05,677 : INFO : PROGRESS: at sentence #450000, processed 4473310 words, keeping 63376 word types\n",
      "2017-04-28 07:31:05,716 : INFO : PROGRESS: at sentence #460000, processed 4571778 words, keeping 63806 word types\n",
      "2017-04-28 07:31:05,741 : INFO : PROGRESS: at sentence #470000, processed 4670379 words, keeping 64234 word types\n",
      "2017-04-28 07:31:05,782 : INFO : PROGRESS: at sentence #480000, processed 4769785 words, keeping 64655 word types\n",
      "2017-04-28 07:31:05,816 : INFO : PROGRESS: at sentence #490000, processed 4868727 words, keeping 65032 word types\n",
      "2017-04-28 07:31:05,841 : INFO : PROGRESS: at sentence #500000, processed 4968327 words, keeping 65422 word types\n",
      "2017-04-28 07:31:05,874 : INFO : PROGRESS: at sentence #510000, processed 5067717 words, keeping 65815 word types\n",
      "2017-04-28 07:31:05,906 : INFO : PROGRESS: at sentence #520000, processed 5166187 words, keeping 66219 word types\n",
      "2017-04-28 07:31:05,931 : INFO : PROGRESS: at sentence #530000, processed 5265570 words, keeping 66600 word types\n",
      "2017-04-28 07:31:05,964 : INFO : PROGRESS: at sentence #540000, processed 5364539 words, keeping 66993 word types\n",
      "2017-04-28 07:31:05,991 : INFO : PROGRESS: at sentence #550000, processed 5463718 words, keeping 67374 word types\n",
      "2017-04-28 07:31:06,019 : INFO : PROGRESS: at sentence #560000, processed 5563461 words, keeping 67784 word types\n",
      "2017-04-28 07:31:06,049 : INFO : PROGRESS: at sentence #570000, processed 5662448 words, keeping 68136 word types\n",
      "2017-04-28 07:31:06,085 : INFO : PROGRESS: at sentence #580000, processed 5761322 words, keeping 68492 word types\n",
      "2017-04-28 07:31:06,113 : INFO : PROGRESS: at sentence #590000, processed 5860695 words, keeping 68849 word types\n",
      "2017-04-28 07:31:06,144 : INFO : PROGRESS: at sentence #600000, processed 5960314 words, keeping 69227 word types\n",
      "2017-04-28 07:31:06,176 : INFO : PROGRESS: at sentence #610000, processed 6060107 words, keeping 69621 word types\n",
      "2017-04-28 07:31:06,205 : INFO : PROGRESS: at sentence #620000, processed 6159312 words, keeping 70001 word types\n",
      "2017-04-28 07:31:06,237 : INFO : PROGRESS: at sentence #630000, processed 6258602 words, keeping 70368 word types\n",
      "2017-04-28 07:31:06,263 : INFO : PROGRESS: at sentence #640000, processed 6357877 words, keeping 70714 word types\n",
      "2017-04-28 07:31:06,308 : INFO : PROGRESS: at sentence #650000, processed 6458132 words, keeping 71063 word types\n",
      "2017-04-28 07:31:06,349 : INFO : PROGRESS: at sentence #660000, processed 6557567 words, keeping 71400 word types\n",
      "2017-04-28 07:31:06,396 : INFO : PROGRESS: at sentence #670000, processed 6655509 words, keeping 71713 word types\n",
      "2017-04-28 07:31:06,436 : INFO : PROGRESS: at sentence #680000, processed 6755049 words, keeping 72057 word types\n",
      "2017-04-28 07:31:06,464 : INFO : PROGRESS: at sentence #690000, processed 6854174 words, keeping 72382 word types\n",
      "2017-04-28 07:31:06,502 : INFO : PROGRESS: at sentence #700000, processed 6953471 words, keeping 72772 word types\n",
      "2017-04-28 07:31:06,529 : INFO : PROGRESS: at sentence #710000, processed 7052430 words, keeping 73100 word types\n",
      "2017-04-28 07:31:06,562 : INFO : PROGRESS: at sentence #720000, processed 7151948 words, keeping 73453 word types\n",
      "2017-04-28 07:31:06,596 : INFO : PROGRESS: at sentence #730000, processed 7250712 words, keeping 73832 word types\n",
      "2017-04-28 07:31:06,629 : INFO : PROGRESS: at sentence #740000, processed 7350937 words, keeping 74152 word types\n",
      "2017-04-28 07:31:06,658 : INFO : PROGRESS: at sentence #750000, processed 7451329 words, keeping 74475 word types\n",
      "2017-04-28 07:31:06,691 : INFO : PROGRESS: at sentence #760000, processed 7550507 words, keeping 74790 word types\n",
      "2017-04-28 07:31:06,736 : INFO : PROGRESS: at sentence #770000, processed 7649836 words, keeping 75107 word types\n",
      "2017-04-28 07:31:06,774 : INFO : PROGRESS: at sentence #780000, processed 7749260 words, keeping 75446 word types\n",
      "2017-04-28 07:31:06,810 : INFO : PROGRESS: at sentence #790000, processed 7848177 words, keeping 75770 word types\n",
      "2017-04-28 07:31:06,851 : INFO : PROGRESS: at sentence #800000, processed 7948114 words, keeping 76062 word types\n",
      "2017-04-28 07:31:06,888 : INFO : PROGRESS: at sentence #810000, processed 8046548 words, keeping 76366 word types\n",
      "2017-04-28 07:31:06,929 : INFO : PROGRESS: at sentence #820000, processed 8146509 words, keeping 76646 word types\n",
      "2017-04-28 07:31:06,961 : INFO : PROGRESS: at sentence #830000, processed 8245800 words, keeping 76949 word types\n",
      "2017-04-28 07:31:06,994 : INFO : PROGRESS: at sentence #840000, processed 8344649 words, keeping 77290 word types\n",
      "2017-04-28 07:31:07,021 : INFO : PROGRESS: at sentence #850000, processed 8444594 words, keeping 77578 word types\n",
      "2017-04-28 07:31:07,051 : INFO : PROGRESS: at sentence #860000, processed 8543987 words, keeping 77887 word types\n",
      "2017-04-28 07:31:07,084 : INFO : PROGRESS: at sentence #870000, processed 8643811 words, keeping 78180 word types\n",
      "2017-04-28 07:31:07,116 : INFO : PROGRESS: at sentence #880000, processed 8743192 words, keeping 78497 word types\n",
      "2017-04-28 07:31:07,157 : INFO : PROGRESS: at sentence #890000, processed 8843552 words, keeping 78790 word types\n",
      "2017-04-28 07:31:07,182 : INFO : PROGRESS: at sentence #900000, processed 8943578 words, keeping 79108 word types\n",
      "2017-04-28 07:31:07,193 : INFO : collected 79225 word types from a corpus of 8976430 raw words and 903280 sentences\n",
      "2017-04-28 07:31:07,194 : INFO : Loading a fresh vocabulary\n",
      "2017-04-28 07:31:07,273 : INFO : min_count=40 retains 9607 unique words (12% of original 79225, drops 69618)\n",
      "2017-04-28 07:31:07,273 : INFO : min_count=40 leaves 8625663 word corpus (96% of original 8976430, drops 350767)\n",
      "2017-04-28 07:31:07,312 : INFO : deleting the raw counts dictionary of 79225 items\n",
      "2017-04-28 07:31:07,319 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2017-04-28 07:31:07,322 : INFO : downsampling leaves estimated 5981220 word corpus (69.3% of prior 8625663)\n",
      "2017-04-28 07:31:07,325 : INFO : estimated required memory for 9607 words and 500 dimensions: 43231500 bytes\n",
      "2017-04-28 07:31:07,379 : INFO : resetting layer weights\n",
      "2017-04-28 07:31:07,565 : INFO : training model with 4 workers on 9607 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-04-28 07:31:07,566 : INFO : expecting 903280 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-28 07:31:08,584 : INFO : PROGRESS: at 1.43% examples, 421916 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:09,610 : INFO : PROGRESS: at 3.23% examples, 473905 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:10,617 : INFO : PROGRESS: at 4.90% examples, 480518 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:11,624 : INFO : PROGRESS: at 6.61% examples, 487235 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:12,624 : INFO : PROGRESS: at 8.55% examples, 504897 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:13,630 : INFO : PROGRESS: at 10.24% examples, 504253 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:14,635 : INFO : PROGRESS: at 12.10% examples, 511193 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:15,664 : INFO : PROGRESS: at 13.99% examples, 516493 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-28 07:31:16,676 : INFO : PROGRESS: at 15.86% examples, 520694 words/s, in_qsize 8, out_qsize 1\n",
      "2017-04-28 07:31:17,680 : INFO : PROGRESS: at 17.66% examples, 522602 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:18,684 : INFO : PROGRESS: at 19.60% examples, 527667 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:19,687 : INFO : PROGRESS: at 21.31% examples, 526173 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:20,695 : INFO : PROGRESS: at 23.10% examples, 526255 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:21,696 : INFO : PROGRESS: at 24.81% examples, 525202 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:22,706 : INFO : PROGRESS: at 26.75% examples, 528277 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:23,728 : INFO : PROGRESS: at 28.68% examples, 530554 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:24,728 : INFO : PROGRESS: at 30.62% examples, 533379 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:25,734 : INFO : PROGRESS: at 32.47% examples, 534378 words/s, in_qsize 7, out_qsize 1\n",
      "2017-04-28 07:31:26,737 : INFO : PROGRESS: at 34.23% examples, 533996 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:27,739 : INFO : PROGRESS: at 36.13% examples, 535596 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:28,750 : INFO : PROGRESS: at 38.08% examples, 537785 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:29,759 : INFO : PROGRESS: at 40.02% examples, 539535 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:30,765 : INFO : PROGRESS: at 41.89% examples, 540149 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:31,774 : INFO : PROGRESS: at 43.80% examples, 541271 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:32,785 : INFO : PROGRESS: at 45.68% examples, 541662 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:33,785 : INFO : PROGRESS: at 47.59% examples, 542745 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:34,802 : INFO : PROGRESS: at 49.35% examples, 541720 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:35,805 : INFO : PROGRESS: at 51.15% examples, 541625 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:36,808 : INFO : PROGRESS: at 52.91% examples, 541098 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:37,814 : INFO : PROGRESS: at 54.67% examples, 540575 words/s, in_qsize 8, out_qsize 1\n",
      "2017-04-28 07:31:38,815 : INFO : PROGRESS: at 56.50% examples, 540791 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-28 07:31:39,831 : INFO : PROGRESS: at 58.21% examples, 539683 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:40,841 : INFO : PROGRESS: at 59.84% examples, 537950 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:41,844 : INFO : PROGRESS: at 61.15% examples, 533646 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:42,848 : INFO : PROGRESS: at 62.49% examples, 529757 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:43,863 : INFO : PROGRESS: at 64.22% examples, 529222 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:44,886 : INFO : PROGRESS: at 66.00% examples, 528929 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:45,905 : INFO : PROGRESS: at 67.61% examples, 527329 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:46,921 : INFO : PROGRESS: at 69.23% examples, 526013 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:47,925 : INFO : PROGRESS: at 71.01% examples, 526144 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:48,954 : INFO : PROGRESS: at 72.58% examples, 524365 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:49,989 : INFO : PROGRESS: at 73.85% examples, 520550 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:31:51,003 : INFO : PROGRESS: at 75.03% examples, 516553 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:52,005 : INFO : PROGRESS: at 76.28% examples, 513333 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:53,021 : INFO : PROGRESS: at 77.54% examples, 510229 words/s, in_qsize 8, out_qsize 1\n",
      "2017-04-28 07:31:54,037 : INFO : PROGRESS: at 78.86% examples, 507543 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:55,045 : INFO : PROGRESS: at 80.17% examples, 505060 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:56,055 : INFO : PROGRESS: at 81.55% examples, 503026 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:31:57,067 : INFO : PROGRESS: at 82.93% examples, 501051 words/s, in_qsize 7, out_qsize 1\n",
      "2017-04-28 07:31:58,083 : INFO : PROGRESS: at 84.35% examples, 499391 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-28 07:31:59,114 : INFO : PROGRESS: at 85.82% examples, 497891 words/s, in_qsize 7, out_qsize 1\n",
      "2017-04-28 07:32:00,119 : INFO : PROGRESS: at 87.27% examples, 496580 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:32:01,147 : INFO : PROGRESS: at 88.70% examples, 494974 words/s, in_qsize 8, out_qsize 2\n",
      "2017-04-28 07:32:02,153 : INFO : PROGRESS: at 90.25% examples, 494379 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:32:03,198 : INFO : PROGRESS: at 91.88% examples, 493869 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-28 07:32:04,199 : INFO : PROGRESS: at 93.55% examples, 493993 words/s, in_qsize 8, out_qsize 1\n",
      "2017-04-28 07:32:05,202 : INFO : PROGRESS: at 95.18% examples, 493855 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-28 07:32:06,210 : INFO : PROGRESS: at 96.83% examples, 493804 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:32:07,220 : INFO : PROGRESS: at 98.54% examples, 494056 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-28 07:32:08,038 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-28 07:32:08,047 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-28 07:32:08,064 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-28 07:32:08,066 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-28 07:32:08,067 : INFO : training on 44882150 raw words (29905741 effective words) took 60.5s, 494406 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 500    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-28 07:32:17,882 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-04-28 07:32:17,947 : INFO : saving Word2Vec object under quora_context, separately None\n",
      "2017-04-28 07:32:17,948 : INFO : not storing attribute syn0norm\n",
      "2017-04-28 07:32:17,950 : INFO : not storing attribute cum_table\n",
      "2017-04-28 07:32:18,440 : INFO : saved quora_context\n"
     ]
    }
   ],
   "source": [
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"quora_context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word2Vec obtained</h2>\n",
    "<p>Now we can use it to find some relation between words</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match([\"java\",\"python\",\"c\",\"javascript\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match([\"dog\",\"elephant\",\"shark\",\"horse\"])\n",
    "#(Dog because pet not wild animal?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coding', 0.7410617470741272),\n",
       " ('python', 0.6689383387565613),\n",
       " ('java', 0.6520576477050781),\n",
       " ('javascript', 0.5605449080467224),\n",
       " ('c', 0.5593013167381287),\n",
       " ('programmer', 0.5227774381637573),\n",
       " ('php', 0.5206589698791504),\n",
       " ('algorithms', 0.5053951740264893),\n",
       " ('linux', 0.501064121723175),\n",
       " ('framework', 0.4715242385864258)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('witch', 0.5302742719650269),\n",
       " ('justified', 0.49773040413856506),\n",
       " ('louisiana', 0.49603551626205444),\n",
       " ('scientology', 0.4932976961135864),\n",
       " ('missouri', 0.4668501615524292),\n",
       " ('majority', 0.4666014015674591),\n",
       " ('racial', 0.464800626039505),\n",
       " ('detroit', 0.4604220688343048),\n",
       " ('wisconsin', 0.459244042634964),\n",
       " ('somalia', 0.4563840627670288)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('danger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unfair', 0.48129281401634216),\n",
       " ('violent', 0.47839125990867615),\n",
       " ('serious', 0.47685545682907104),\n",
       " ('diplomatic', 0.4541323482990265),\n",
       " ('prevalent', 0.4492725431919098),\n",
       " ('liberal', 0.4460740089416504),\n",
       " ('controversial', 0.44428908824920654),\n",
       " ('bullies', 0.44424617290496826),\n",
       " ('conservative', 0.44246068596839905),\n",
       " ('ignorant', 0.4410874545574188)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ok', 0.9252203106880188),\n",
       " ('acceptable', 0.6984410881996155),\n",
       " ('advisable', 0.6913833022117615),\n",
       " ('normal', 0.6496853828430176),\n",
       " ('safe', 0.6454579830169678),\n",
       " ('necessary', 0.6417340040206909),\n",
       " ('possible', 0.6039286851882935),\n",
       " ('fine', 0.5829481482505798),\n",
       " ('weird', 0.5782525539398193),\n",
       " ('appropriate', 0.5637184381484985)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('okay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Read more : https://www.kaggle.com/c/word2vec-nlp-tutorial</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
