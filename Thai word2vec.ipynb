{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word2Vec (Thai word)</h1>\n",
    "<p>In this notebook, we will try to construct a word2vec from a thai corpus</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "file = codecs.open('dataset/orchid97-utf8.crp','r','utf-8')\n",
    "fileString = file.read()\n",
    "testArr = fileString.split(\"#\")\n",
    "testArr = [row for row in testArr if '/' in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "wordsList = []\n",
    "\n",
    "for row in testArr:\n",
    "    parts = row.split('//')\n",
    "    if(\"\\n\" in parts[0] and len(parts)>1):\n",
    "        sentence = parts[0].split(\"\\n\")[1]\n",
    "        sentence = sentence.replace(\" \",\"\")\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        partsArr = parts[1].split(\"\\n\")\n",
    "        partsArr = [p.split(\"/\")[0] for p in partsArr]\n",
    "        partsArr = [p for p in partsArr if p!='<space>']\n",
    "        wordsList.append(partsArr[1:-1])\n",
    "\n",
    "tokenized_sentences = [\" \".join(word) for word in wordsList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['การ', 'ประชุม', 'ทาง', 'วิชาการ', 'ครั้ง', 'ที่ 1'],\n",
       " ['โครงการวิจัยและพัฒนา', 'อิเล็กทรอนิกส์', 'และ', 'คอมพิวเตอร์'],\n",
       " ['ปีงบประมาณ', '2531'],\n",
       " ['เล่ม', '1'],\n",
       " ['ศูนย์เทคโนโลยีอิเล็กทรอนิกส์และคอมพิวเตอร์แห่งชาติ']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-04-28 08:01:26,696 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-04-28 08:01:26,702 : INFO : collecting all words and their counts\n",
      "2017-04-28 08:01:26,704 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-28 08:01:26,731 : INFO : PROGRESS: at sentence #10000, processed 118968 words, keeping 9396 word types\n",
      "2017-04-28 08:01:26,760 : INFO : PROGRESS: at sentence #20000, processed 229315 words, keeping 15114 word types\n",
      "2017-04-28 08:01:26,776 : INFO : collected 17185 word types from a corpus of 288841 raw words and 24879 sentences\n",
      "2017-04-28 08:01:26,777 : INFO : Loading a fresh vocabulary\n",
      "2017-04-28 08:01:26,789 : INFO : min_count=40 retains 815 unique words (4% of original 17185, drops 16370)\n",
      "2017-04-28 08:01:26,790 : INFO : min_count=40 leaves 233066 word corpus (80% of original 288841, drops 55775)\n",
      "2017-04-28 08:01:26,794 : INFO : deleting the raw counts dictionary of 17185 items\n",
      "2017-04-28 08:01:26,796 : INFO : sample=0.001 downsamples 71 most-common words\n",
      "2017-04-28 08:01:26,797 : INFO : downsampling leaves estimated 158855 word corpus (68.2% of prior 233066)\n",
      "2017-04-28 08:01:26,798 : INFO : estimated required memory for 815 words and 500 dimensions: 3667500 bytes\n",
      "2017-04-28 08:01:26,802 : INFO : resetting layer weights\n",
      "2017-04-28 08:01:26,821 : INFO : training model with 4 workers on 815 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-04-28 08:01:26,822 : INFO : expecting 24879 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-28 08:01:27,744 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-28 08:01:27,748 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-28 08:01:27,752 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-28 08:01:27,756 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-28 08:01:27,757 : INFO : training on 1444205 raw words (794203 effective words) took 0.9s, 858647 effective words/s\n",
      "2017-04-28 08:01:27,758 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-04-28 08:01:27,765 : INFO : saving Word2Vec object under telex_context, separately None\n",
      "2017-04-28 08:01:27,766 : INFO : not storing attribute syn0norm\n",
      "2017-04-28 08:01:27,768 : INFO : not storing attribute cum_table\n",
      "2017-04-28 08:01:27,807 : INFO : saved telex_context\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 500    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(wordsList, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"telex_context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('สารนิเทศ', 0.9880535006523132),\n",
       " ('มหาวิทยาลัย', 0.9817095398902893),\n",
       " ('สถาบัน', 0.9740793108940125),\n",
       " ('สาขา', 0.9730427265167236),\n",
       " ('วารสาร', 0.9635933041572571),\n",
       " ('ห้องสมุด', 0.9582613110542297),\n",
       " ('เครือข่ายคอมพิวเตอร์', 0.952242374420166),\n",
       " ('แห่ง', 0.9518786668777466),\n",
       " ('ทรัพยากร', 0.9424996376037598),\n",
       " ('NECTEC', 0.9415864944458008)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('วิทยาศาสตร์')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ศูนย์เทคโนโลยีอิเล็กทรอนิกส์และคอมพิวเตอร์แห่งชาติ', 0.9383119344711304),\n",
       " ('เสนอ', 0.9129125475883484),\n",
       " ('คณะ', 0.9037826061248779),\n",
       " ('อิเล็กทรอนิกส์', 0.9027509689331055),\n",
       " ('ปี', 0.901549220085144),\n",
       " ('เครือข่ายคอมพิวเตอร์', 0.8999563455581665),\n",
       " ('สารนิเทศ', 0.891416609287262),\n",
       " ('สนับสนุน', 0.8882615566253662),\n",
       " ('วิชาการ', 0.8881869912147522),\n",
       " ('NECTEC', 0.8856964707374573)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('โครงการ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('เครือข่ายคอมพิวเตอร์', 0.9833523035049438),\n",
       " ('นโยบาย', 0.971004068851471),\n",
       " ('งบประมาณ', 0.9627659320831299),\n",
       " ('จัดตั้ง', 0.9621474742889404),\n",
       " ('มหาวิทยาลัย', 0.9554195404052734),\n",
       " ('ทรัพยากร', 0.9538665413856506),\n",
       " ('ร่วม', 0.9534153342247009),\n",
       " ('เอกชน', 0.9524781107902527),\n",
       " ('สถาบัน', 0.9502981901168823),\n",
       " ('สารนิเทศ', 0.9473577737808228)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('NECTEC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('วิชาการ', 0.9838699102401733),\n",
       " ('ครั้ง', 0.935802161693573),\n",
       " ('ปีงบประมาณ', 0.9299863576889038),\n",
       " ('2532', 0.9235069751739502),\n",
       " ('วิศวกรรมไฟฟ้า', 0.9214415550231934),\n",
       " ('ศูนย์เทคโนโลยีอิเล็กทรอนิกส์และคอมพิวเตอร์แห่งชาติ', 0.9207188487052917),\n",
       " ('ภาควิชา', 0.9121106863021851),\n",
       " ('มหาวิทยาลัยเชียงใหม่', 0.9048099517822266),\n",
       " ('ผลงานวิจัย', 0.9021814465522766),\n",
       " ('2536', 0.8995680809020996)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('ประชุม')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Some word might not exist in vocab</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d3b3b4045577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'เกม'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mode' is not defined"
     ]
    }
   ],
   "source": [
    "mode.most_similar('เกม')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TODO</h3>\n",
    "<ul>\n",
    "<li>Implement word2vec to PokemonGO corpus using thai segment</li>\n",
    "<li>Comparison between bag of words & word2vec</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
