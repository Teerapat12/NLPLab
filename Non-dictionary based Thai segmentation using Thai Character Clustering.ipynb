{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Thai segmentation using TCC</h1>\n",
    "<p>In this python notebook, I'll try to demonstrate how to \n",
    "convert string to TCC form and apply machine learning technique to create a model to predict whether\n",
    "2 TCC should be combine or not</p>\n",
    "\n",
    "<p>First let's import library and dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#Import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "file = codecs.open('dataset/orchid97-utf8.crp','r','utf-8')\n",
    "fileString = file.read()\n",
    "testArr = fileString.split(\"#\")\n",
    "testArr = [row for row in testArr if '/' in row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>First let's take a look at the data</h2>\n",
    "<p>For each row, we will have a sentence, and each of the word in sentence tagged with part of speech.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\nการประชุมทางวิชาการ ครั้งที่ 1//\\nการ/FIXN\\nประชุม/VACT\\nทาง/NCMN\\nวิชาการ/NCMN\\n<space>/PUNC\\nครั้ง/CFQC\\nที่ 1/DONM\\n//\\n',\n",
       " '2\\nโครงการวิจัยและพัฒนาอิเล็กทรอนิกส์และคอมพิวเตอร์//\\nโครงการวิจัยและพัฒนา/NCMN\\nอิเล็กทรอนิกส์/NCMN\\nและ/JCRG\\nคอมพิวเตอร์/NCMN\\n//\\n',\n",
       " '3\\nปีงบประมาณ 2531//\\nปีงบประมาณ/NCMN\\n<space>/PUNC\\n2531/NCNM\\n//\\n',\n",
       " '4\\nเล่ม 1//\\nเล่ม/CNIT\\n<space>/PUNC\\n1/DONM\\n//\\n',\n",
       " '1\\nศูนย์เทคโนโลยีอิเล็กทรอนิกส์และคอมพิวเตอร์แห่งชาติ//\\nศูนย์เทคโนโลยีอิเล็กทรอนิกส์และคอมพิวเตอร์แห่งชาติ/NPRP\\n//\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testArr[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Let's convert this to a pandas DataFrame so that we can work with it easier.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert string of each row to arrays.\n",
    "sentences = []\n",
    "wordsList = []\n",
    "for row in testArr:\n",
    "    parts = row.split('//')\n",
    "    if(\"\\n\" in parts[0] and len(parts)>1):\n",
    "        sentence = parts[0].split(\"\\n\")[1]\n",
    "        sentence = sentence.replace(\" \",\"\")\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        partsArr = parts[1].split(\"\\n\")\n",
    "        partsArr = [p.split(\"/\")[0] for p in partsArr]\n",
    "        partsArr = [p for p in partsArr if p!='<space>']\n",
    "        wordsList.append(partsArr[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences example : ['การประชุมทางวิชาการครั้งที่1', 'โครงการวิจัยและพัฒนาอิเล็กทรอนิกส์และคอมพิวเตอร์']\n",
      "Words List example : [['การ', 'ประชุม', 'ทาง', 'วิชาการ', 'ครั้ง', 'ที่ 1'], ['โครงการวิจัยและพัฒนา', 'อิเล็กทรอนิกส์', 'และ', 'คอมพิวเตอร์']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences example : \",end='')\n",
    "print(sentences[0:2])\n",
    "print(\"Words List example : \",end='')\n",
    "print(wordsList[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>More on TCC</h2>\n",
    "<p>In Thai language, some contiguous characters tend to be an\n",
    "inseparable unit, called Thai character cluster (TCC). Unlike word\n",
    "segmentation that is a very difficult task, segmenting a text into\n",
    "TCCs is easily realized by applying a set of rules. This method\n",
    "needs no dictionary and can always correctly segment a text at\n",
    "every word boundaries.</p>\n",
    "\n",
    "<p>Read more at : http://wing.comp.nus.edu.sg/~antho/H/H01/H01-1057.pdf</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'กา|ร|เก็|บ|ภา|ษี|ป|ระ|เท|ศ|ไท|ย|และ|ป|ระ|เท|ศ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string2TCC(sentence):\n",
    "    consonants = 'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮ'\n",
    "    upper_vowel = '็   ิ ี ึ ํ'\n",
    "    lower_vowel = 'ุู'\n",
    "    front_vowel = 'เแโใไ'\n",
    "    rear_vowel = 'าําๅๆะฯๅๆ'\n",
    "    word = ''\n",
    "    words = []\n",
    "    #หา consonants ตัด consonants ตัวต่อไปที่เจอ\n",
    "    for i in range(len(sentence)):\n",
    "        s = sentence[i]\n",
    "        if(s in front_vowel or (s in consonants and (sentence[i-1] not in front_vowel and sentence[i-1] != 'ั')) or s==' '):\n",
    "            words.append(word)\n",
    "            word = ''\n",
    "        if(s!=' '):\n",
    "            word += s\n",
    "    words.append(word)\n",
    "    return words[1:]\n",
    "\"|\".join(string2TCC('การเก็บภาษีประเทศไทยและประเทศ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>New dataset construction</h2>\n",
    "<p>Now we will construct a dataset consist of 4 features and 1 label</p>\n",
    "<p>The 4 features will be preWord, word1,word2,sufWord in TCC form</p>\n",
    "<p>The label will be whether we should combine TCC into 1 word or not.</p>\n",
    "<p>With this we can use some machine learning technique to create a model to predict whether we should combine any given 4 TCC or not</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_data_sample(sentences,wordsList):\n",
    "#    TCC = string2TCC(sentence)\n",
    "    features_1 = []\n",
    "    features_2 = []\n",
    "    features_3 = []\n",
    "    features_4 = []\n",
    "    labels = []\n",
    "\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        TCCS = string2TCC(sentence)\n",
    "        for j,TCC in enumerate(TCCS[:-3]):\n",
    "            next1 = TCCS[j+1]\n",
    "            next2 = TCCS[j+2]\n",
    "            next3 = TCCS[j+3]\n",
    "            #Check if we should combine next1 and next2\n",
    "            #Check if TCC+next1 is in the same word and next2+next3 is in same word and next1/next2 is in different one\n",
    "            firstHalf = TCC+next1\n",
    "            secondHalf = next2+next3\n",
    "            words = wordsList[i]\n",
    "            features_1.append(TCC)\n",
    "            features_2.append(next1)\n",
    "            features_3.append(next2)\n",
    "            features_4.append(next3)\n",
    "            labelValue = 1\n",
    "            for k,word in enumerate(words[:-1]):\n",
    "                nextWord = words[k+1]\n",
    "                if (firstHalf in word) and (secondHalf in nextWord) and (next1+next2 not in word) and (next1+next2 not in words[k+1]):\n",
    "                    labelValue = 0\n",
    "                    break\n",
    "            labels.append(labelValue)        \n",
    "    TCC_corpus = pd.DataFrame({'preWord':features_1,'word1':features_2,'word2':features_3,'sufWord':features_4,'shouldCombine':labels},  columns=['preWord','word1','word2','sufWord','shouldCombine'])\n",
    "    return TCC_corpus\n",
    "df = construct_data_sample(sentences,wordsList)\n",
    "#Save file in case we want to use it later.\n",
    "df.to_csv('dataset/TCCs_corpus.csv',index=False ,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>New Dataset obtained</h2>\n",
    "<p>Let's look more at our new construct dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preWord</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sufWord</th>\n",
       "      <th>shouldCombine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ร</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>ทา</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>ทา</td>\n",
       "      <td>ง</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  preWord word1 word2 sufWord  shouldCombine\n",
       "0      กา     ร     ป      ระ              0\n",
       "1       ร     ป    ระ      ชุ              1\n",
       "2       ป    ระ    ชุ       ม              1\n",
       "3      ระ    ชุ     ม      ทา              1\n",
       "4      ชุ     ม    ทา       ง              0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612226, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data preprocess (again).</h2>\n",
    "<p>Now let's do some more data preprocess with this new dataset.</p>\n",
    "<p>For now let's extract feature propose by [1]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consonants = 'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮ'\n",
    "upper_vowel = '็   ิ ี ึ ํ'\n",
    "lower_vowel = 'ุู'\n",
    "front_vowel = 'เแโใไ'\n",
    "rear_vowel = 'าําๅๆะฯๅๆ'\n",
    "kok = ['ก', 'ข' ,'ค', 'ฆ']  # kok\n",
    "kod = ['จ', 'ด', 'ต', 'ถ', 'ท', 'ธ', 'ฎ', 'ฏ', 'ฑ', 'ฒ', 'ช', 'ซ', 'ศ', 'ษ' ,'ส']  # กด\n",
    "kob = ['บ','ป' ,'พ', 'ภ', 'ฟ'] \n",
    "kon = ['น', 'ณ', 'ญ', 'ร' ,'ล','ฬ'] \n",
    "kong = ['ง'] \n",
    "kom = ['ม ']\n",
    "keiy = ['ย']\n",
    "keve = ['ว']\n",
    "feature_names=  ['fv','fc','mv','mc','rv','rc','length','spaceEnter']\n",
    "wordTypeName= ['preWord_','word1_','word2_','sufWords_']\n",
    "middleConsonantGroup = [kok,kod,kob,kon,kong,kom,keiy,keve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 2, 0, 2, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract TCC from given TCC\n",
    "def TCC2features(TCC):    \n",
    "    fv = 0\n",
    "    fc = 0\n",
    "    mv = 0\n",
    "    mc = 0\n",
    "    rv = 0\n",
    "    for i,s in enumerate(TCC):\n",
    "        #Front Vowel\n",
    "        if(s in front_vowel):\n",
    "            fv = 1\n",
    "        #check for Front Consonant\n",
    "        if(s=='ห' or s=='อ' and (TCC[i-1] not in consonants or i-1<0)):\n",
    "            fc = 2\n",
    "        elif(s in consonants):\n",
    "            fc = 1\n",
    "        #Check for Middle Vowel\n",
    "        if(s in upper_vowel):\n",
    "            mv = 1\n",
    "        elif(s in lower_vowel):\n",
    "            mv = 2\n",
    "        #Check for Middle Consonant\n",
    "        if(s in consonants and TCC[i-1] in consonants and i-1>0):\n",
    "            mc = 1\n",
    "        #Check for rear_vowel\n",
    "        if(s=='ะ'):\n",
    "            rv = 1\n",
    "        elif(s=='า' or s=='ำ'):\n",
    "            rv = 2\n",
    "        #Check rear consonant\n",
    "        #end without consonant\n",
    "    lastChar = TCC[-1]\n",
    "    rc = 9\n",
    "    if(lastChar not in consonants): rc= 0 \n",
    "    for j,group in enumerate(middleConsonantGroup):\n",
    "        if lastChar in group:\n",
    "            rc = j+1\n",
    "    length = len(TCC)\n",
    "    spaceEnter = 0 #Maybe change in da future\n",
    "    \n",
    "    return [fv,fc,mv,mc,rv,rc,length,spaceEnter]\n",
    "TCC2features('กา')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preWord</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sufWord</th>\n",
       "      <th>shouldCombine</th>\n",
       "      <th>preWord_fv</th>\n",
       "      <th>preWord_fc</th>\n",
       "      <th>preWord_mv</th>\n",
       "      <th>preWord_mc</th>\n",
       "      <th>preWord_rv</th>\n",
       "      <th>...</th>\n",
       "      <th>word2_length</th>\n",
       "      <th>word2_spaceEnter</th>\n",
       "      <th>sufWords_fv</th>\n",
       "      <th>sufWords_fc</th>\n",
       "      <th>sufWords_mv</th>\n",
       "      <th>sufWords_mc</th>\n",
       "      <th>sufWords_rv</th>\n",
       "      <th>sufWords_rc</th>\n",
       "      <th>sufWords_length</th>\n",
       "      <th>sufWords_spaceEnter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ร</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>ทา</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>ทา</td>\n",
       "      <td>ง</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  preWord word1 word2 sufWord  shouldCombine  preWord_fv  preWord_fc  \\\n",
       "0      กา     ร     ป      ระ              0           0           1   \n",
       "1       ร     ป    ระ      ชุ              1           0           1   \n",
       "2       ป    ระ    ชุ       ม              1           0           1   \n",
       "3      ระ    ชุ     ม      ทา              1           0           1   \n",
       "4      ชุ     ม    ทา       ง              0           0           1   \n",
       "\n",
       "   preWord_mv  preWord_mc  preWord_rv         ...           word2_length  \\\n",
       "0           0           0           2         ...                      1   \n",
       "1           0           0           0         ...                      2   \n",
       "2           0           0           0         ...                      2   \n",
       "3           0           0           1         ...                      1   \n",
       "4           2           0           0         ...                      2   \n",
       "\n",
       "   word2_spaceEnter  sufWords_fv  sufWords_fc  sufWords_mv  sufWords_mc  \\\n",
       "0                 0            0            1            0            0   \n",
       "1                 0            0            1            2            0   \n",
       "2                 0            0            1            0            0   \n",
       "3                 0            0            1            0            0   \n",
       "4                 0            0            1            0            0   \n",
       "\n",
       "   sufWords_rv  sufWords_rc  sufWords_length  sufWords_spaceEnter  \n",
       "0            1            0                2                    0  \n",
       "1            0            0                2                    0  \n",
       "2            0            9                1                    0  \n",
       "3            2            0                2                    0  \n",
       "4            0            5                1                    0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_features(row):\n",
    "    preWordFeatures = TCC2features(row['preWord'])\n",
    "    word1Features = TCC2features(row['word1'])\n",
    "    word2Features = TCC2features(row['word2'])\n",
    "    sufWordsFeatures = TCC2features(row['sufWord'])\n",
    "    for k,features in enumerate([preWordFeatures,word1Features,word2Features,sufWordsFeatures]):\n",
    "        feature_prefix = wordTypeName[k]\n",
    "        for i in range(8):\n",
    "            row[feature_prefix+feature_names[i]] = features[i]\n",
    "    return row\n",
    "df.head().apply(get_features,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Do this with the rest</h2>\n",
    "<p>Now we'll apply them to all of our dataset (65555 rows)</p>\n",
    "<p>This might take some times</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time : 2.85s.\n"
     ]
    }
   ],
   "source": [
    "#The one below is the finished version of csv.\n",
    "start = timeit.default_timer()\n",
    "#features_dataset =  df.apply(get_features,axis=1)\n",
    "features_dataset = pd.read_csv('dataset/TCC_Features.csv',encoding = \"utf-8\")\n",
    "stop = timeit.default_timer()\n",
    "print(\"Execution time : \"+str(round(stop - start,2))+\"s.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's see how much can ANN predict this dataset.</h2>\n",
    "<p>First, set X and y for training in ANN.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's look at only 10k because it might take too long.\n",
    "features_dataset = features_dataset.head(10000)\n",
    "features = ['preWord_fv','preWord_fc', 'preWord_mv', 'preWord_mc', 'preWord_rv', 'preWord_rc',\n",
    "       'preWord_length',  'word1_fv', 'word1_fc',\n",
    "       'word1_mv', 'word1_mc', 'word1_rv', 'word1_rc', 'word1_length',\n",
    "        'word2_fv', 'word2_fc', 'word2_mv', 'word2_mc',\n",
    "       'word2_rv', 'word2_rc', 'word2_length', \n",
    "       'sufWords_fv', 'sufWords_fc', 'sufWords_mv', 'sufWords_mc',\n",
    "       'sufWords_rv', 'sufWords_rc', 'sufWords_length']\n",
    "target = ['shouldCombine']\n",
    "X = features_dataset[features].values\n",
    "y = features_dataset[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now let's look at the accuracy ANN can product</h2>\n",
    "<p>This might even be longer than extracting features.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Real implementation were done in spyder. \n",
    "Toomuch computation for jupyter\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim=16,init='uniform',activation='relu',input_dim=31))    \n",
    "    classifier.add(Dense(output_dim=16,init='uniform',activation='relu'))    \n",
    "    classifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))\n",
    "    classifier.compile(optimizer= 'adam',loss='binary_crossentropy',metrics=['accuracy']) #categorical_crossentropy\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size=10,nb_epoch=100)\n",
    "accuracies = cross_val_score(estimator= classifier, X = X_train,y= y_train, cv =10)\n",
    "mean = accuracies.mean()\n",
    "std = accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "classifier = load_model('tcc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1score = f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 89.05%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \"+str((cm[0][0]+cm[1][1])*100/(cm[0][0]+cm[1][1]+cm[1][0]+cm[0][1]))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.938706968934\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score : \"+str(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Evaluating\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim=6,init='uniform',activation='relu',input_dim=11))    \n",
    "    classifier.add(Dense(output_dim=6,init='uniform',activation='relu'))    \n",
    "    classifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))\n",
    "    classifier.compile(optimizer= 'adam',loss='binary_crossentropy',metrics=['accuracy']) #categorical_crossentropy\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size=10,nb_epoch=100)\n",
    "accuracies = cross_val_score(estimator= classifier, X = X_train,y= y_train, cv =10)\n",
    "mean = accuracies.mean()\n",
    "std = accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = features_dataset.sample(30)\n",
    "sample_X = sample[features].values\n",
    "sample_y = sample[target].values\n",
    "sample_pred = classifier.predict(sample_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_pred(y,y_pred):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred = (y_pred>0.5)\n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1score = f1_score(y_test,y_pred)\n",
    "    print(\"Accuracy : \"+str((cm[0][0]+cm[1][1])*100/(cm[0][0]+cm[1][1]+cm[1][0]+cm[0][1]))+\"%\")\n",
    "    print(\"F1 score : \"+str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 89.05%\n",
      "F1 score : 0.938706968934\n"
     ]
    }
   ],
   "source": [
    "evaluate_pred(sample_pred,sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample['shouldCombinePred'] = sample_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preWord</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sufWord</th>\n",
       "      <th>shouldCombine</th>\n",
       "      <th>shouldCombinePred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7154</th>\n",
       "      <td>ใน</td>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>ป</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9076</th>\n",
       "      <td>ร</td>\n",
       "      <td>ร</td>\n",
       "      <td>ว</td>\n",
       "      <td>ม</td>\n",
       "      <td>1</td>\n",
       "      <td>0.958510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>ร</td>\n",
       "      <td>แป</td>\n",
       "      <td>ล</td>\n",
       "      <td>เอ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8780</th>\n",
       "      <td>ด</td>\n",
       "      <td>เป็</td>\n",
       "      <td>น</td>\n",
       "      <td>สา</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9957</th>\n",
       "      <td>บ</td>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>เจื</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9961</th>\n",
       "      <td>อ</td>\n",
       "      <td>สา</td>\n",
       "      <td>ร</td>\n",
       "      <td>นั้</td>\n",
       "      <td>1</td>\n",
       "      <td>0.974557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>ว</td>\n",
       "      <td>คื</td>\n",
       "      <td>อ</td>\n",
       "      <td>ผู้</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8714</th>\n",
       "      <td>พาะ</td>\n",
       "      <td>ใน</td>\n",
       "      <td>บ</td>\n",
       "      <td>ริ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.942895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>ระ</td>\n",
       "      <td>ดิ</td>\n",
       "      <td>ษ</td>\n",
       "      <td>ฐ์(Artificialintel&lt;LI&gt;gence)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>ผู้</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ก</td>\n",
       "      <td>1</td>\n",
       "      <td>0.704274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>จา</td>\n",
       "      <td>ก</td>\n",
       "      <td>ส</td>\n",
       "      <td>ถา</td>\n",
       "      <td>0</td>\n",
       "      <td>0.732309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7665</th>\n",
       "      <td>โท</td>\n",
       "      <td>ร</td>\n",
       "      <td>ศัพ</td>\n",
       "      <td>ท์(662)248-8078-84</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>เร็</td>\n",
       "      <td>จ</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ดับ</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9353</th>\n",
       "      <td>อ</td>\n",
       "      <td>อ</td>\n",
       "      <td>ก</td>\n",
       "      <td>แบ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5285</th>\n",
       "      <td>อิ</td>\n",
       "      <td>เล็</td>\n",
       "      <td>ก</td>\n",
       "      <td>ท</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8451</th>\n",
       "      <td>เห</td>\n",
       "      <td>ล่า</td>\n",
       "      <td>นี้</td>\n",
       "      <td>ขึ้</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>ค</td>\n",
       "      <td>ลา</td>\n",
       "      <td>ก</td>\n",
       "      <td>ร</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4650</th>\n",
       "      <td>อ</td>\n",
       "      <td>ก</td>\n",
       "      <td>จา</td>\n",
       "      <td>ก</td>\n",
       "      <td>1</td>\n",
       "      <td>0.735644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>ง</td>\n",
       "      <td>ค์</td>\n",
       "      <td>ดัง</td>\n",
       "      <td>ก</td>\n",
       "      <td>0</td>\n",
       "      <td>0.830691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6156</th>\n",
       "      <td>ลัก</td>\n",
       "      <td>ษ</td>\n",
       "      <td>ณะ</td>\n",
       "      <td>ข</td>\n",
       "      <td>1</td>\n",
       "      <td>0.793792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>อ</td>\n",
       "      <td>ฟ</td>\n",
       "      <td>ท์</td>\n",
       "      <td>แว</td>\n",
       "      <td>1</td>\n",
       "      <td>0.940523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6485</th>\n",
       "      <td>ภา</td>\n",
       "      <td>ษา</td>\n",
       "      <td>เป้า</td>\n",
       "      <td>ห</td>\n",
       "      <td>1</td>\n",
       "      <td>0.725365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292</th>\n",
       "      <td>ค</td>\n",
       "      <td>ณะ</td>\n",
       "      <td>ก</td>\n",
       "      <td>ร</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>ร</td>\n",
       "      <td>เส</td>\n",
       "      <td>น</td>\n",
       "      <td>อ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>วา</td>\n",
       "      <td>ม</td>\n",
       "      <td>สำ</td>\n",
       "      <td>คัญ</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077</th>\n",
       "      <td>ลิ</td>\n",
       "      <td>ต</td>\n",
       "      <td>ภัณ</td>\n",
       "      <td>ฑ์</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>ร</td>\n",
       "      <td>ร</td>\n",
       "      <td>ม</td>\n",
       "      <td>อิ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.782370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9950</th>\n",
       "      <td>บ</td>\n",
       "      <td>กับ</td>\n",
       "      <td>ผู้</td>\n",
       "      <td>ป</td>\n",
       "      <td>1</td>\n",
       "      <td>0.775518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>ง</td>\n",
       "      <td>ที่</td>\n",
       "      <td>สำ</td>\n",
       "      <td>คัญ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.920592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5444</th>\n",
       "      <td>ร</td>\n",
       "      <td>ง</td>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>1</td>\n",
       "      <td>0.741132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     preWord word1 word2                       sufWord  shouldCombine  \\\n",
       "7154      ใน    กา     ร                             ป              1   \n",
       "9076       ร     ร     ว                             ม              1   \n",
       "5903       ร    แป     ล                            เอ              1   \n",
       "8780       ด   เป็     น                            สา              1   \n",
       "9957       บ    กา     ร                           เจื              1   \n",
       "9961       อ    สา     ร                           นั้              1   \n",
       "9982       ว    คื     อ                           ผู้              1   \n",
       "8714     พาะ    ใน     บ                            ริ              1   \n",
       "5363      ระ    ดิ     ษ  ฐ์(Artificialintel<LI>gence)              1   \n",
       "4667     ผู้     ป    ระ                             ก              1   \n",
       "5822      จา     ก     ส                            ถา              0   \n",
       "7665      โท     ร   ศัพ            ท์(662)248-8078-84              1   \n",
       "996      เร็     จ    ระ                           ดับ              0   \n",
       "9353       อ     อ     ก                            แบ              1   \n",
       "5285      อิ   เล็     ก                             ท              1   \n",
       "8451      เห   ล่า   นี้                           ขึ้              1   \n",
       "6786       ค    ลา     ก                             ร              1   \n",
       "4650       อ     ก    จา                             ก              1   \n",
       "190        ง    ค์   ดัง                             ก              0   \n",
       "6156     ลัก     ษ    ณะ                             ข              1   \n",
       "5382       อ     ฟ    ท์                            แว              1   \n",
       "6485      ภา    ษา  เป้า                             ห              1   \n",
       "4292       ค    ณะ     ก                             ร              0   \n",
       "3737       ร    เส     น                             อ              1   \n",
       "894       วา     ม    สำ                           คัญ              0   \n",
       "7077      ลิ     ต   ภัณ                            ฑ์              1   \n",
       "1410       ร     ร     ม                            อิ              1   \n",
       "9950       บ   กับ   ผู้                             ป              1   \n",
       "1913       ง   ที่    สำ                           คัญ              1   \n",
       "5444       ร     ง    กา                             ร              1   \n",
       "\n",
       "      shouldCombinePred  \n",
       "7154           0.975347  \n",
       "9076           0.958510  \n",
       "5903           0.996851  \n",
       "8780           0.998248  \n",
       "9957           0.999829  \n",
       "9961           0.974557  \n",
       "9982           0.907214  \n",
       "8714           0.942895  \n",
       "5363           1.000000  \n",
       "4667           0.704274  \n",
       "5822           0.732309  \n",
       "7665           0.999999  \n",
       "996            0.747341  \n",
       "9353           0.996079  \n",
       "5285           0.994309  \n",
       "8451           0.939536  \n",
       "6786           0.983442  \n",
       "4650           0.735644  \n",
       "190            0.830691  \n",
       "6156           0.793792  \n",
       "5382           0.940523  \n",
       "6485           0.725365  \n",
       "4292           0.943745  \n",
       "3737           0.941530  \n",
       "894            0.395108  \n",
       "7077           0.776744  \n",
       "1410           0.782370  \n",
       "9950           0.775518  \n",
       "1913           0.920592  \n",
       "5444           0.741132  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[['preWord','word1','word2','sufWord', 'shouldCombine','shouldCombinePred']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>New ANN Model obtain!</h2>\n",
    "<p>Now we got a ANN that can predict whether 2 TCC should be combine or not with accuracy of ~90%</p>\n",
    "<p>Let's try to use the classifier to tokenize Thai word</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['อยากไปเรียน', 'วิทยาศาสตร์ที่ไทย']\n",
      "['ไปหาเพื่อน', 'ที่สยาม']\n",
      "['โครงการ', 'ศึกษาที่สาม']\n"
     ]
    }
   ],
   "source": [
    "sentences = ['อยากไปเรียนวิทยาศาสตร์ที่ไทย','ไปหาเพื่อนที่สยาม','โครงการศึกษาที่สาม']\n",
    "def segment(sentence,classifier=classifier):\n",
    "    #Step 1 : Change string to TCC\n",
    "    TCCs = string2TCC(sentence)\n",
    "    #Step 2 : construct unlabled dataset \n",
    "    preWord = []\n",
    "    word1 = []\n",
    "    word2 = []\n",
    "    sufWord = []\n",
    "    for i in range(len(TCCs)-3):\n",
    "        [w1,w2,w3,w4] = TCCs[i:i+4] \n",
    "        preWord.append(w1)\n",
    "        word1.append(w2)\n",
    "        word2.append(w3)\n",
    "        sufWord.append(w4)\n",
    "    df = pd.DataFrame({\"preWord\":preWord,\"word1\":word1,\"word2\":word2,\"sufWord\":sufWord} ,columns=['preWord','word1','word2','sufWord'])\n",
    "    #Step 3 : Extract features from dataset\n",
    "    feature_df = df.apply(get_features,axis=1)\n",
    "    X = feature_df[features].values\n",
    "    #Step 4 predict using ANN\n",
    "    y_pred = classifier.predict(X)\n",
    "    df['shouldCombine'] = y_pred\n",
    "    #Step 5 : Segment word by predicted result\n",
    "    wordsList = []\n",
    "    word=TCCs[0]+TCCs[1]\n",
    "    for i in range(len(y_pred)):\n",
    "        preWord = TCCs[i]\n",
    "        word1 = TCCs[i+1]\n",
    "        word2 = TCCs[i+2]\n",
    "        sufWord = TCCs[i+3]\n",
    "        if(y_pred[i]<0.60):\n",
    "            #ตัดคำ\n",
    "            wordsList.append(word)\n",
    "            word = ''\n",
    "        word+=word2 \n",
    "    word +=TCCs[-1]\n",
    "    wordsList.append(word)\n",
    "    return wordsList\n",
    "for sentence in sentences:\n",
    "    print(segment(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "<p>With this approah, we  can seperate thai word by using deep learning model.</p>\n",
    "<p>If we have more free text, then the model might be more accurace.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Referecenes</h3>\n",
    "<ul>\n",
    "<li>Non-dictionary-based Thai word segmentation using decision trees by Thanaruk Theeramunkong : http://globe2.thaicyberu.go.th/node/1915763</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
