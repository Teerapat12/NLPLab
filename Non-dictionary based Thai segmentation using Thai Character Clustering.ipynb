{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Thai segmentation using TCC</h1>\n",
    "<p>In this python notebook, I'll try to demonstrate how to \n",
    "convert string to TCC form and apply machine learning technique to create a model to predict whether\n",
    "2 TCC should be combine or not</p>\n",
    "\n",
    "<p>First let's import library and dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#Import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "file = codecs.open('dataset/orchid97-utf8.crp','r','utf-8')\n",
    "fileString = file.read()\n",
    "testArr = fileString.split(\"#\")\n",
    "testArr = [row for row in testArr if '/' in row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>First let's take a look at the data</h2>\n",
    "<p>For each row, we will have a sentence, and each of the word in sentence tagged with part of speech.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\nการประชุมทางวิชาการ ครั้งที่ 1//\\nการ/FIXN\\nประชุม/VACT\\nทาง/NCMN\\nวิชาการ/NCMN\\n<space>/PUNC\\nครั้ง/CFQC\\nที่ 1/DONM\\n//\\n',\n",
       " '2\\nโครงการวิจัยและพัฒนาอิเล็กทรอนิกส์และคอมพิวเตอร์//\\nโครงการวิจัยและพัฒนา/NCMN\\nอิเล็กทรอนิกส์/NCMN\\nและ/JCRG\\nคอมพิวเตอร์/NCMN\\n//\\n',\n",
       " '3\\nปีงบประมาณ 2531//\\nปีงบประมาณ/NCMN\\n<space>/PUNC\\n2531/NCNM\\n//\\n',\n",
       " '4\\nเล่ม 1//\\nเล่ม/CNIT\\n<space>/PUNC\\n1/DONM\\n//\\n',\n",
       " '1\\nศูนย์เทคโนโลยีอิเล็กทรอนิกส์และคอมพิวเตอร์แห่งชาติ//\\nศูนย์เทคโนโลยีอิเล็กทรอนิกส์และคอมพิวเตอร์แห่งชาติ/NPRP\\n//\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testArr[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Let's convert this to a pandas DataFrame so that we can work with it easier.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert string of each row to arrays.\n",
    "sentences = []\n",
    "wordsList = []\n",
    "for row in testArr:\n",
    "    parts = row.split('//')\n",
    "    if(\"\\n\" in parts[0] and len(parts)>1):\n",
    "        sentence = parts[0].split(\"\\n\")[1]\n",
    "        sentence = sentence.replace(\" \",\"\")\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        partsArr = parts[1].split(\"\\n\")\n",
    "        partsArr = [p.split(\"/\")[0] for p in partsArr]\n",
    "        partsArr = [p for p in partsArr if p!='<space>']\n",
    "        wordsList.append(partsArr[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences example : ['การประชุมทางวิชาการครั้งที่1', 'โครงการวิจัยและพัฒนาอิเล็กทรอนิกส์และคอมพิวเตอร์']\n",
      "Words List example : [['การ', 'ประชุม', 'ทาง', 'วิชาการ', 'ครั้ง', 'ที่ 1'], ['โครงการวิจัยและพัฒนา', 'อิเล็กทรอนิกส์', 'และ', 'คอมพิวเตอร์']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences example : \",end='')\n",
    "print(sentences[0:2])\n",
    "print(\"Words List example : \",end='')\n",
    "print(wordsList[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>More on TCC</h2>\n",
    "<p>In Thai language, some contiguous characters tend to be an\n",
    "inseparable unit, called Thai character cluster (TCC). Unlike word\n",
    "segmentation that is a very difficult task, segmenting a text into\n",
    "TCCs is easily realized by applying a set of rules. This method\n",
    "needs no dictionary and can always correctly segment a text at\n",
    "every word boundaries.</p>\n",
    "\n",
    "<p>Read more at : http://wing.comp.nus.edu.sg/~antho/H/H01/H01-1057.pdf</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'กา|ร|เก็|บ|ภา|ษี|ป|ระ|เท|ศ|ไท|ย|และ|ป|ระ|เท|ศ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string2TCC(sentence):\n",
    "    consonants = 'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮ'\n",
    "    upper_vowel = '็   ิ ี ึ ํ'\n",
    "    lower_vowel = 'ุู'\n",
    "    front_vowel = 'เแโใไ'\n",
    "    rear_vowel = 'าําๅๆะฯๅๆ'\n",
    "    word = ''\n",
    "    words = []\n",
    "    #หา consonants ตัด consonants ตัวต่อไปที่เจอ\n",
    "    for i in range(len(sentence)):\n",
    "        s = sentence[i]\n",
    "        if(s in front_vowel or (s in consonants and (sentence[i-1] not in front_vowel and sentence[i-1] != 'ั')) or s==' '):\n",
    "            words.append(word)\n",
    "            word = ''\n",
    "        if(s!=' '):\n",
    "            word += s\n",
    "    words.append(word)\n",
    "    return words[1:]\n",
    "\"|\".join(string2TCC('การเก็บภาษีประเทศไทยและประเทศ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>New dataset construction</h2>\n",
    "<p>Now we will construct a dataset consist of 4 features and 1 label</p>\n",
    "<p>The 4 features will be preWord, word1,word2,sufWord in TCC form</p>\n",
    "<p>The label will be whether we should combine TCC into 1 word or not.</p>\n",
    "<p>With this we can use some machine learning technique to create a model to predict whether we should combine any given 4 TCC or not</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_data_sample(sentences,wordsList):\n",
    "#    TCC = string2TCC(sentence)\n",
    "    features_1 = []\n",
    "    features_2 = []\n",
    "    features_3 = []\n",
    "    features_4 = []\n",
    "    labels = []\n",
    "\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        TCCS = string2TCC(sentence)\n",
    "        for j,TCC in enumerate(TCCS[:-3]):\n",
    "            next1 = TCCS[j+1]\n",
    "            next2 = TCCS[j+2]\n",
    "            next3 = TCCS[j+3]\n",
    "            #Check if we should combine next1 and next2\n",
    "            #Check if TCC+next1 is in the same word and next2+next3 is in same word and next1/next2 is in different one\n",
    "            firstHalf = TCC+next1\n",
    "            secondHalf = next2+next3\n",
    "            words = wordsList[i]\n",
    "            features_1.append(TCC)\n",
    "            features_2.append(next1)\n",
    "            features_3.append(next2)\n",
    "            features_4.append(next3)\n",
    "            labelValue = 1\n",
    "            for k,word in enumerate(words[:-1]):\n",
    "                nextWord = words[k+1]\n",
    "                if (firstHalf in word) and (secondHalf in nextWord) and (next1+next2 not in word) and (next1+next2 not in words[k+1]):\n",
    "                    labelValue = 0\n",
    "                    break\n",
    "            labels.append(labelValue)        \n",
    "    TCC_corpus = pd.DataFrame({'preWord':features_1,'word1':features_2,'word2':features_3,'sufWord':features_4,'shouldCombine':labels},  columns=['preWord','word1','word2','sufWord','shouldCombine'])\n",
    "    return TCC_corpus\n",
    "df = construct_data_sample(sentences,wordsList)\n",
    "#Save file in case we want to use it later.\n",
    "df.to_csv('dataset/TCCs_corpus.csv',index=False ,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>New Dataset obtained</h2>\n",
    "<p>Let's look more at our new construct dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preWord</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sufWord</th>\n",
       "      <th>shouldCombine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ร</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>ทา</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>ทา</td>\n",
       "      <td>ง</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  preWord word1 word2 sufWord  shouldCombine\n",
       "0      กา     ร     ป      ระ              0\n",
       "1       ร     ป    ระ      ชุ              1\n",
       "2       ป    ระ    ชุ       ม              1\n",
       "3      ระ    ชุ     ม      ทา              1\n",
       "4      ชุ     ม    ทา       ง              0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612226, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data preprocess (again).</h2>\n",
    "<p>Now let's do some more data preprocess with this new dataset.</p>\n",
    "<p>For now let's extract feature propose by [1]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consonants = 'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮ'\n",
    "upper_vowel = '็   ิ ี ึ ํ'\n",
    "lower_vowel = 'ุู'\n",
    "front_vowel = 'เแโใไ'\n",
    "rear_vowel = 'าําๅๆะฯๅๆ'\n",
    "kok = ['ก', 'ข' ,'ค', 'ฆ']  # kok\n",
    "kod = ['จ', 'ด', 'ต', 'ถ', 'ท', 'ธ', 'ฎ', 'ฏ', 'ฑ', 'ฒ', 'ช', 'ซ', 'ศ', 'ษ' ,'ส']  # กด\n",
    "kob = ['บ','ป' ,'พ', 'ภ', 'ฟ'] \n",
    "kon = ['น', 'ณ', 'ญ', 'ร' ,'ล','ฬ'] \n",
    "kong = ['ง'] \n",
    "kom = ['ม ']\n",
    "keiy = ['ย']\n",
    "keve = ['ว']\n",
    "feature_names=  ['fv','fc','mv','mc','rv','rc','length','spaceEnter']\n",
    "wordTypeName= ['preWord_','word1_','word2_','sufWords_']\n",
    "middleConsonantGroup = [kok,kod,kob,kon,kong,kom,keiy,keve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 2, 0, 2, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract TCC from given TCC\n",
    "def TCC2features(TCC):    \n",
    "    fv = 0\n",
    "    fc = 0\n",
    "    mv = 0\n",
    "    mc = 0\n",
    "    rv = 0\n",
    "    for i,s in enumerate(TCC):\n",
    "        #Front Vowel\n",
    "        if(s in front_vowel):\n",
    "            fv = 1\n",
    "        #check for Front Consonant\n",
    "        if(s=='ห' or s=='อ' and (TCC[i-1] not in consonants or i-1<0)):\n",
    "            fc = 2\n",
    "        elif(s in consonants):\n",
    "            fc = 1\n",
    "        #Check for Middle Vowel\n",
    "        if(s in upper_vowel):\n",
    "            mv = 1\n",
    "        elif(s in lower_vowel):\n",
    "            mv = 2\n",
    "        #Check for Middle Consonant\n",
    "        if(s in consonants and TCC[i-1] in consonants and i-1>0):\n",
    "            mc = 1\n",
    "        #Check for rear_vowel\n",
    "        if(s=='ะ'):\n",
    "            rv = 1\n",
    "        elif(s=='า' or s=='ำ'):\n",
    "            rv = 2\n",
    "        #Check rear consonant\n",
    "        #end without consonant\n",
    "    lastChar = TCC[-1]\n",
    "    rc = 9\n",
    "    if(lastChar not in consonants): rc= 0 \n",
    "    for j,group in enumerate(middleConsonantGroup):\n",
    "        if lastChar in group:\n",
    "            rc = j+1\n",
    "    length = len(TCC)\n",
    "    spaceEnter = 0 #Maybe change in da future\n",
    "    \n",
    "    return [fv,fc,mv,mc,rv,rc,length,spaceEnter]\n",
    "TCC2features('กา')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preWord</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sufWord</th>\n",
       "      <th>shouldCombine</th>\n",
       "      <th>preWord_fv</th>\n",
       "      <th>preWord_fc</th>\n",
       "      <th>preWord_mv</th>\n",
       "      <th>preWord_mc</th>\n",
       "      <th>preWord_rv</th>\n",
       "      <th>...</th>\n",
       "      <th>word2_length</th>\n",
       "      <th>word2_spaceEnter</th>\n",
       "      <th>sufWords_fv</th>\n",
       "      <th>sufWords_fc</th>\n",
       "      <th>sufWords_mv</th>\n",
       "      <th>sufWords_mc</th>\n",
       "      <th>sufWords_rv</th>\n",
       "      <th>sufWords_rc</th>\n",
       "      <th>sufWords_length</th>\n",
       "      <th>sufWords_spaceEnter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ร</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ระ</td>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>ทา</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ชุ</td>\n",
       "      <td>ม</td>\n",
       "      <td>ทา</td>\n",
       "      <td>ง</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  preWord word1 word2 sufWord  shouldCombine  preWord_fv  preWord_fc  \\\n",
       "0      กา     ร     ป      ระ              0           0           1   \n",
       "1       ร     ป    ระ      ชุ              1           0           1   \n",
       "2       ป    ระ    ชุ       ม              1           0           1   \n",
       "3      ระ    ชุ     ม      ทา              1           0           1   \n",
       "4      ชุ     ม    ทา       ง              0           0           1   \n",
       "\n",
       "   preWord_mv  preWord_mc  preWord_rv         ...           word2_length  \\\n",
       "0           0           0           2         ...                      1   \n",
       "1           0           0           0         ...                      2   \n",
       "2           0           0           0         ...                      2   \n",
       "3           0           0           1         ...                      1   \n",
       "4           2           0           0         ...                      2   \n",
       "\n",
       "   word2_spaceEnter  sufWords_fv  sufWords_fc  sufWords_mv  sufWords_mc  \\\n",
       "0                 0            0            1            0            0   \n",
       "1                 0            0            1            2            0   \n",
       "2                 0            0            1            0            0   \n",
       "3                 0            0            1            0            0   \n",
       "4                 0            0            1            0            0   \n",
       "\n",
       "   sufWords_rv  sufWords_rc  sufWords_length  sufWords_spaceEnter  \n",
       "0            1            0                2                    0  \n",
       "1            0            0                2                    0  \n",
       "2            0            9                1                    0  \n",
       "3            2            0                2                    0  \n",
       "4            0            5                1                    0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_features(row):\n",
    "    preWordFeatures = TCC2features(row['preWord'])\n",
    "    word1Features = TCC2features(row['word1'])\n",
    "    word2Features = TCC2features(row['word2'])\n",
    "    sufWordsFeatures = TCC2features(row['sufWord'])\n",
    "    for k,features in enumerate([preWordFeatures,word1Features,word2Features,sufWordsFeatures]):\n",
    "        feature_prefix = wordTypeName[k]\n",
    "        for i in range(8):\n",
    "            row[feature_prefix+feature_names[i]] = features[i]\n",
    "    return row\n",
    "df.head().apply(get_features,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Do this with the rest</h2>\n",
    "<p>Now we'll apply them to all of our dataset (65555 rows)</p>\n",
    "<p>This might take some times</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time : 3.89s.\n"
     ]
    }
   ],
   "source": [
    "#The one below is the finished version of csv.\n",
    "start = timeit.default_timer()\n",
    "#features_dataset =  df.apply(get_features,axis=1)\n",
    "features_dataset = pd.read_csv('dataset/TCC_Features.csv',encoding = \"utf-8\")\n",
    "stop = timeit.default_timer()\n",
    "print(\"Execution time : \"+str(round(stop - start,2))+\"s.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's see how much can ANN predict this dataset.</h2>\n",
    "<p>First, set X and y for training in ANN.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's look at only 10k because it might take too long.\n",
    "features_dataset = features_dataset.head(10000)\n",
    "features = ['preWord_fv','preWord_fc', 'preWord_mv', 'preWord_mc', 'preWord_rv', 'preWord_rc',\n",
    "       'preWord_length',  'word1_fv', 'word1_fc',\n",
    "       'word1_mv', 'word1_mc', 'word1_rv', 'word1_rc', 'word1_length',\n",
    "        'word2_fv', 'word2_fc', 'word2_mv', 'word2_mc',\n",
    "       'word2_rv', 'word2_rc', 'word2_length', \n",
    "       'sufWords_fv', 'sufWords_fc', 'sufWords_mv', 'sufWords_mc',\n",
    "       'sufWords_rv', 'sufWords_rc', 'sufWords_length']\n",
    "target = ['shouldCombine']\n",
    "X = features_dataset[features].values\n",
    "y = features_dataset[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now let's look at the accuracy ANN can product</h2>\n",
    "<p>This might even be longer than extracting features.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Real implementation were done in spyder. \n",
    "Toomuch computation for jupyter\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim=16,init='uniform',activation='relu',input_dim=31))    \n",
    "    classifier.add(Dense(output_dim=16,init='uniform',activation='relu'))    \n",
    "    classifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))\n",
    "    classifier.compile(optimizer= 'adam',loss='binary_crossentropy',metrics=['accuracy']) #categorical_crossentropy\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size=10,nb_epoch=100)\n",
    "accuracies = cross_val_score(estimator= classifier, X = X_train,y= y_train, cv =10)\n",
    "mean = accuracies.mean()\n",
    "std = accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "classifier = load_model('tcc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 28)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1score = f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 88.45%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \"+str((cm[0][0]+cm[1][1])*100/(cm[0][0]+cm[1][1]+cm[1][0]+cm[0][1]))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.935057632837\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score : \"+str(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Evaluating\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim=6,init='uniform',activation='relu',input_dim=11))    \n",
    "    classifier.add(Dense(output_dim=6,init='uniform',activation='relu'))    \n",
    "    classifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))\n",
    "    classifier.compile(optimizer= 'adam',loss='binary_crossentropy',metrics=['accuracy']) #categorical_crossentropy\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size=10,nb_epoch=100)\n",
    "accuracies = cross_val_score(estimator= classifier, X = X_train,y= y_train, cv =10)\n",
    "mean = accuracies.mean()\n",
    "std = accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = features_dataset.sample(30)\n",
    "sample_X = sample[features].values\n",
    "sample_y = sample[target].values\n",
    "sample_pred = classifier.predict(sample_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_pred(y,y_pred):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred = (y_pred>0.5)\n",
    "    # Making the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1score = f1_score(y_test,y_pred)\n",
    "    print(\"Accuracy : \"+str((cm[0][0]+cm[1][1])*100/(cm[0][0]+cm[1][1]+cm[1][0]+cm[0][1]))+\"%\")\n",
    "    print(\"F1 score : \"+str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 89.05%\n",
      "F1 score : 0.938706968934\n"
     ]
    }
   ],
   "source": [
    "evaluate_pred(sample_pred,sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample['shouldCombinePred'] = sample_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preWord</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sufWord</th>\n",
       "      <th>shouldCombine</th>\n",
       "      <th>shouldCombinePred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7297</th>\n",
       "      <td>ฝ่า</td>\n",
       "      <td>ย</td>\n",
       "      <td>ที่</td>\n",
       "      <td>เกี่</td>\n",
       "      <td>1</td>\n",
       "      <td>0.872496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789</th>\n",
       "      <td>ร</td>\n",
       "      <td>ด้า</td>\n",
       "      <td>น</td>\n",
       "      <td>วิ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.986215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5577</th>\n",
       "      <td>ร</td>\n",
       "      <td>ร</td>\n",
       "      <td>ม</td>\n",
       "      <td>เค</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>เท</td>\n",
       "      <td>ค</td>\n",
       "      <td>โน</td>\n",
       "      <td>โล</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9072</th>\n",
       "      <td>บ</td>\n",
       "      <td>ว</td>\n",
       "      <td>ง</td>\n",
       "      <td>จ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.912067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7584</th>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>วิ</td>\n",
       "      <td>จัย</td>\n",
       "      <td>1</td>\n",
       "      <td>0.745945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9452</th>\n",
       "      <td>บ</td>\n",
       "      <td>ลึ</td>\n",
       "      <td>ก</td>\n",
       "      <td>ล</td>\n",
       "      <td>1</td>\n",
       "      <td>0.952630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5372</th>\n",
       "      <td>นา</td>\n",
       "      <td>ค</td>\n",
       "      <td>อ</td>\n",
       "      <td>ม</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6597</th>\n",
       "      <td>ลา</td>\n",
       "      <td>ค</td>\n",
       "      <td>ม-</td>\n",
       "      <td>ปัจ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>ว</td>\n",
       "      <td>ย55</td>\n",
       "      <td>โค</td>\n",
       "      <td>ร</td>\n",
       "      <td>1</td>\n",
       "      <td>0.742357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>เท</td>\n",
       "      <td>ค</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>น</td>\n",
       "      <td>กับ</td>\n",
       "      <td>ภา</td>\n",
       "      <td>ค</td>\n",
       "      <td>1</td>\n",
       "      <td>0.822905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8504</th>\n",
       "      <td>สั่</td>\n",
       "      <td>ง</td>\n",
       "      <td>ซื้</td>\n",
       "      <td>อ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8042</th>\n",
       "      <td>ร</td>\n",
       "      <td>อ</td>\n",
       "      <td>อ</td>\n",
       "      <td>ก</td>\n",
       "      <td>1</td>\n",
       "      <td>0.974063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>ค</td>\n",
       "      <td>โน</td>\n",
       "      <td>โล</td>\n",
       "      <td>ยี</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>ระ</td>\n",
       "      <td>มา</td>\n",
       "      <td>ณ2531</td>\n",
       "      <td>ที่</td>\n",
       "      <td>1</td>\n",
       "      <td>0.986291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>น</td>\n",
       "      <td>ก</td>\n",
       "      <td>ร</td>\n",
       "      <td>ร</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8202</th>\n",
       "      <td>ง</td>\n",
       "      <td>ค์</td>\n",
       "      <td>ห</td>\n",
       "      <td>ลัก</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>และ</td>\n",
       "      <td>สา</td>\n",
       "      <td>มา</td>\n",
       "      <td>ร</td>\n",
       "      <td>1</td>\n",
       "      <td>0.775119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>ว</td>\n",
       "      <td>ง</td>\n",
       "      <td>วิ</td>\n",
       "      <td>ท</td>\n",
       "      <td>1</td>\n",
       "      <td>0.613347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>ต่า</td>\n",
       "      <td>ง</td>\n",
       "      <td>ป</td>\n",
       "      <td>ระ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.412592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6040</th>\n",
       "      <td>ลัก</td>\n",
       "      <td>ดัง</td>\n",
       "      <td>ก</td>\n",
       "      <td>ล่า</td>\n",
       "      <td>1</td>\n",
       "      <td>0.861148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>ก</td>\n",
       "      <td>ระ</td>\n",
       "      <td>ท</td>\n",
       "      <td>ร</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>ย</td>\n",
       "      <td>อ</td>\n",
       "      <td>ม</td>\n",
       "      <td>ร</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9877</th>\n",
       "      <td>ย</td>\n",
       "      <td>ใน</td>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>1</td>\n",
       "      <td>0.930155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5966</th>\n",
       "      <td>เท</td>\n",
       "      <td>ศ(MITI)</td>\n",
       "      <td>ข</td>\n",
       "      <td>อ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369</th>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>พัฒ</td>\n",
       "      <td>นา</td>\n",
       "      <td>0</td>\n",
       "      <td>0.413412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>โด</td>\n",
       "      <td>ย</td>\n",
       "      <td>กา</td>\n",
       "      <td>ร</td>\n",
       "      <td>0</td>\n",
       "      <td>0.413873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>น</td>\n",
       "      <td>งา</td>\n",
       "      <td>น</td>\n",
       "      <td>วิ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>ง</td>\n",
       "      <td>แผ</td>\n",
       "      <td>น</td>\n",
       "      <td>และ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     preWord    word1  word2 sufWord  shouldCombine  shouldCombinePred\n",
       "7297     ฝ่า        ย    ที่    เกี่              1           0.872496\n",
       "6789       ร      ด้า      น      วิ              1           0.986215\n",
       "5577       ร        ร      ม      เค              1           0.998410\n",
       "7922      เท        ค     โน      โล              1           0.850626\n",
       "9072       บ        ว      ง       จ              1           0.912067\n",
       "7584      กา        ร     วิ     จัย              1           0.745945\n",
       "9452       บ       ลึ      ก       ล              1           0.952630\n",
       "5372      นา        ค      อ       ม              1           0.957613\n",
       "6597      ลา        ค     ม-     ปัจ              1           0.851791\n",
       "1680       ว      ย55     โค       ร              1           0.742357\n",
       "3522      กา        ร     เท       ค              0           0.376413\n",
       "1847       น      กับ     ภา       ค              1           0.822905\n",
       "8504     สั่        ง    ซื้       อ              1           0.375847\n",
       "8042       ร        อ      อ       ก              1           0.974063\n",
       "902        ค       โน     โล      ยี              1           0.957333\n",
       "1637      ระ       มา  ณ2531     ที่              1           0.986291\n",
       "2941       น        ก      ร       ร              1           0.938769\n",
       "8202       ง       ค์      ห     ลัก              0           0.930177\n",
       "553      และ       สา     มา       ร              1           0.775119\n",
       "4064       ว        ง     วิ       ท              1           0.613347\n",
       "482      ต่า        ง      ป      ระ              1           0.412592\n",
       "6040     ลัก      ดัง      ก     ล่า              1           0.861148\n",
       "2749       ก       ระ      ท       ร              1           0.950279\n",
       "2732       ย        อ      ม       ร              1           0.991576\n",
       "9877       ย       ใน     กา       ร              1           0.930155\n",
       "5966      เท  ศ(MITI)      ข       อ              1           0.889221\n",
       "5369      กา        ร    พัฒ      นา              0           0.413412\n",
       "401       โด        ย     กา       ร              0           0.413873\n",
       "1820       น       งา      น      วิ              1           0.985790\n",
       "719        ง       แผ      น     และ              1           0.999493"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[['preWord','word1','word2','sufWord', 'shouldCombine','shouldCombinePred']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>New ANN Model obtain!</h2>\n",
    "<p>Now we got a ANN that can predict whether 2 TCC should be combine or not with accuracy of ~90%</p>\n",
    "<p>Let's try to use the classifier to tokenize Thai word</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['อ', 'ยา', 'ก', 'ไป', 'เรี', 'ย', 'น', 'วิ', 'ท', 'ยา', 'ศา', 'ส', 'ต', 'ร์', 'ที่', 'ไท', 'ย']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preWord</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sufWord</th>\n",
       "      <th>shouldCombine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>อ</td>\n",
       "      <td>ยา</td>\n",
       "      <td>ก</td>\n",
       "      <td>ไป</td>\n",
       "      <td>0.998697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ยา</td>\n",
       "      <td>ก</td>\n",
       "      <td>ไป</td>\n",
       "      <td>เรี</td>\n",
       "      <td>0.789984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ก</td>\n",
       "      <td>ไป</td>\n",
       "      <td>เรี</td>\n",
       "      <td>ย</td>\n",
       "      <td>0.892810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ไป</td>\n",
       "      <td>เรี</td>\n",
       "      <td>ย</td>\n",
       "      <td>น</td>\n",
       "      <td>0.997113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>เรี</td>\n",
       "      <td>ย</td>\n",
       "      <td>น</td>\n",
       "      <td>วิ</td>\n",
       "      <td>0.752717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ย</td>\n",
       "      <td>น</td>\n",
       "      <td>วิ</td>\n",
       "      <td>ท</td>\n",
       "      <td>0.362348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>น</td>\n",
       "      <td>วิ</td>\n",
       "      <td>ท</td>\n",
       "      <td>ยา</td>\n",
       "      <td>0.912517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>วิ</td>\n",
       "      <td>ท</td>\n",
       "      <td>ยา</td>\n",
       "      <td>ศา</td>\n",
       "      <td>0.745370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ท</td>\n",
       "      <td>ยา</td>\n",
       "      <td>ศา</td>\n",
       "      <td>ส</td>\n",
       "      <td>0.882278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ยา</td>\n",
       "      <td>ศา</td>\n",
       "      <td>ส</td>\n",
       "      <td>ต</td>\n",
       "      <td>0.950653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ศา</td>\n",
       "      <td>ส</td>\n",
       "      <td>ต</td>\n",
       "      <td>ร์</td>\n",
       "      <td>0.797738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ส</td>\n",
       "      <td>ต</td>\n",
       "      <td>ร์</td>\n",
       "      <td>ที่</td>\n",
       "      <td>0.950862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ต</td>\n",
       "      <td>ร์</td>\n",
       "      <td>ที่</td>\n",
       "      <td>ไท</td>\n",
       "      <td>0.892652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ร์</td>\n",
       "      <td>ที่</td>\n",
       "      <td>ไท</td>\n",
       "      <td>ย</td>\n",
       "      <td>0.850378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preWord word1 word2 sufWord  shouldCombine\n",
       "0        อ    ยา     ก      ไป       0.998697\n",
       "1       ยา     ก    ไป     เรี       0.789984\n",
       "2        ก    ไป   เรี       ย       0.892810\n",
       "3       ไป   เรี     ย       น       0.997113\n",
       "4      เรี     ย     น      วิ       0.752717\n",
       "5        ย     น    วิ       ท       0.362348\n",
       "6        น    วิ     ท      ยา       0.912517\n",
       "7       วิ     ท    ยา      ศา       0.745370\n",
       "8        ท    ยา    ศา       ส       0.882278\n",
       "9       ยา    ศา     ส       ต       0.950653\n",
       "10      ศา     ส     ต      ร์       0.797738\n",
       "11       ส     ต    ร์     ที่       0.950862\n",
       "12       ต    ร์   ที่      ไท       0.892652\n",
       "13      ร์   ที่    ไท       ย       0.850378"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'อยากไปเรียนวิทยาศาสตร์ที่ไทย'\n",
    "#Step 1 : Change string to TCC\n",
    "TCCs = string2TCC(sentence)\n",
    "print(TCCs)\n",
    "#Step 2 : construct unlabled dataset \n",
    "preWord = []\n",
    "word1 = []\n",
    "word2 = []\n",
    "sufWord = []\n",
    "for i in range(len(TCCs)-3):\n",
    "    [w1,w2,w3,w4] = TCCs[i:i+4] \n",
    "    preWord.append(w1)\n",
    "    word1.append(w2)\n",
    "    word2.append(w3)\n",
    "    sufWord.append(w4)\n",
    "    \n",
    "df = pd.DataFrame({\"preWord\":preWord,\"word1\":word1,\"word2\":word2,\"sufWord\":sufWord} ,columns=['preWord','word1','word2','sufWord'])\n",
    "#Step 3 : Extract features from dataset\n",
    "feature_df = df.apply(get_features,axis=1)\n",
    "X = feature_df[features].values\n",
    "#Step 4 predict using ANN\n",
    "y_pred = classifier.predict(X)\n",
    "df['shouldCombine'] = y_pred\n",
    "#Step 5 : Segment word by predicted result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
